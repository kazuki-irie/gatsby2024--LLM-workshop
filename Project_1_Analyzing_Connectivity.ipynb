{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Connectivity project"
      ],
      "metadata": {
        "id": "lD9a5EJ_rIZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load packages"
      ],
      "metadata": {
        "id": "WDt3oPojV62p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V034T0K_rE_5"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/EleutherAI/lm-evaluation-harness --depth 1\n",
        "%cd lm-evaluation-harness\n",
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "FHqAkfgnVsZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -i https://pypi.org/simple/ bitsandbytes"
      ],
      "metadata": {
        "id": "yIdMIe19VxZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "A7OOCAWyVx6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import lm_eval\n",
        "import transformers"
      ],
      "metadata": {
        "id": "vmh3ZaAVUzTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "2MjE4IEJV3o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download pre-trained model"
      ],
      "metadata": {
        "id": "2ZoFsn3kWBF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "bgeF3F6jyYb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"EleutherAI/pythia-2.8b\"\n",
        "# model_name = \"tiiuae/falcon-7b\"\n",
        "# model_name = \"lmsys/vicuna-7b-v1.5\"\n",
        "# model_name = \"EleutherAI/pythia-6.9b\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "fNyW4CTfWEde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "model.save_pretrained('./local-example-llm')\n",
        "tokenizer.save_pretrained(\"./local-example-llm/tokenizer/\")"
      ],
      "metadata": {
        "id": "oTklOSMYWNli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./local-example-llm"
      ],
      "metadata": {
        "id": "wwMWeytM58xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "D9CsCWYhWbJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specified models\n",
        "\n",
        "!{sys.executable} -m lm_eval --model hf \\\n",
        "    --model_args pretrained=./local-example-llm,tokenizer=./local-example-llm/tokenizer,dtype=\"float16\" \\\n",
        "    --tasks lambada_openai,hellaswag \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto:4"
      ],
      "metadata": {
        "id": "5DCjjT7wWZOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and prepare \"Vicuna\" dataset\n",
        "https://lmsys.org/blog/2023-03-30-vicuna/"
      ],
      "metadata": {
        "id": "l3dFAfmdWd3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/lm-sys/FastChat/main/fastchat/llm_judge/data/vicuna_bench/question.jsonl"
      ],
      "metadata": {
        "id": "mNyylyaHWiBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "file_path = './question.jsonl'\n",
        "\n",
        "with open(file_path, 'r') as json_file:\n",
        "    json_list = list(json_file)\n",
        "\n",
        "vicuna_dataset = {}\n",
        "\n",
        "for json_str in json_list:\n",
        "    result = json.loads(json_str)\n",
        "    # turns, categories, question_id\n",
        "    prompt = result['turns'][0]\n",
        "    assert len(result['turns']) == 1\n",
        "    category = result['category']\n",
        "    if not category in vicuna_dataset:\n",
        "        vicuna_dataset[category] = []\n",
        "    vicuna_dataset[category].append(prompt)"
      ],
      "metadata": {
        "id": "gkkt8h3UW9GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vicuna_dataset.keys())"
      ],
      "metadata": {
        "id": "bracXekdW-1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vicuna_dataset['math'][1])"
      ],
      "metadata": {
        "id": "BDA2682XXHk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example evaluation"
      ],
      "metadata": {
        "id": "T6k9vYuyXhvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = vicuna_dataset['math'][1]\n",
        "\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "out = model.generate(input_ids['input_ids'], max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
        "result = tokenizer.decode(out[0])\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "5vaVcNpBXlee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example code for pruning"
      ],
      "metadata": {
        "id": "VO172aDZWFz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple magnitude-based pruning method. Code taken from: https://github.com/locuslab/wanda\n",
        "\n",
        "def find_layers(module, layers=[nn.Linear], name=''):\n",
        "    \"\"\"\n",
        "    Recursively find the layers of a certain type in a module.\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): PyTorch module.\n",
        "        layers (list): List of layer types to find.\n",
        "        name (str): Name of the module.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of layers of the given type(s) within the module.\n",
        "    \"\"\"\n",
        "    if type(module) in layers:\n",
        "        return {name: module}\n",
        "    res = {}\n",
        "    for name1, child in module.named_children():\n",
        "        res.update(find_layers(\n",
        "            child, layers=layers, name=name + '.' + name1 if name != '' else name1\n",
        "        ))\n",
        "    return res\n",
        "\n",
        "def prune_magnitude(model, tokenizer, device=torch.device(\"cuda:0\"), sparsity_ratio=0, prune_n=0, prune_m=0):\n",
        "    if hasattr(model, 'base_model'):\n",
        "      if hasattr(model.base_model, 'layers'):\n",
        "          layers = model.base_model.layers\n",
        "      else:\n",
        "          layers = model.base_model._modules\n",
        "    else:\n",
        "      layers = model.model.layers\n",
        "\n",
        "    for i in range(len(layers)):\n",
        "        layer = layers[i]\n",
        "        subset = find_layers(layer)\n",
        "\n",
        "        for name in subset:\n",
        "            W = subset[name].weight.data\n",
        "            W_metric = torch.abs(W)\n",
        "            if prune_n != 0:\n",
        "                W_mask = (torch.zeros_like(W)==1)\n",
        "                for ii in range(W_metric.shape[1]):\n",
        "                    if ii % prune_m == 0:\n",
        "                        tmp = W_metric[:,ii:(ii+prune_m)].float()\n",
        "                        W_mask.scatter_(1,ii+torch.topk(tmp, prune_n,dim=1, largest=False)[1], True)\n",
        "            else:\n",
        "                thresh = torch.sort(W_metric.flatten().cuda())[0][int(W.numel()*sparsity_ratio)].cpu()\n",
        "                W_mask = (W_metric<=thresh)\n",
        "\n",
        "            W[W_mask] = 0"
      ],
      "metadata": {
        "id": "74bBS5hlb0Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.base_model.layers"
      ],
      "metadata": {
        "id": "myyHa-eRvuqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.base_model._modules"
      ],
      "metadata": {
        "id": "gEItDYLvwUrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply pruning, save the pruned model, evaluate it"
      ],
      "metadata": {
        "id": "QXqvtg06XvC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prune_magnitude(model, tokenizer, sparsity_ratio=0.4)"
      ],
      "metadata": {
        "id": "LNYXocnTb3Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./local-example-pruned-llm\n",
        "model.save_pretrained('./local-example-pruned-llm')\n",
        "tokenizer.save_pretrained(\"./local-example-pruned-llm/tokenizer/\")"
      ],
      "metadata": {
        "id": "9SgYaCCafcS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m lm_eval --model hf \\\n",
        "    --model_args pretrained=./local-example-pruned-llm,tokenizer=./local-example-pruned-llm/tokenizer \\\n",
        "    --tasks lambada_openai,hellaswag \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto:4"
      ],
      "metadata": {
        "id": "iWayDKErloTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = vicuna_dataset['math'][1]\n",
        "\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "out = model.generate(input_ids['input_ids'], max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
        "result = tokenizer.decode(out[0])\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "9rl_nQNDwyMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qV_dzzH7c8qX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}